{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf0399-2deb-47cc-b9d5-c57cb13e1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install pillow\n",
    "!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beff04ea-87c5-4148-88a1-738581059cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")  # Should match your NVCC (12.8)\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62887e5-15f1-4672-a03b-60985ab61bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4373f7dad3b2490a8555ca20f56639c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n",
      "VRAM usage: 15.14GB\n",
      "Processing minute 0...\n",
      "Processing minute 1...\n",
      "Processing minute 2...\n",
      "Processing minute 3...\n",
      "Processing minute 4...\n",
      "Processing minute 5...\n",
      "Processing minute 6...\n",
      "Processing minute 7...\n",
      "Processing minute 8...\n",
      "Processing minute 9...\n",
      "Processing minute 10...\n",
      "Processing minute 11...\n",
      "Processing minute 12...\n",
      "Processing minute 13...\n",
      "Processing minute 14...\n",
      "Processing minute 15...\n",
      "Processing minute 16...\n",
      "Processing minute 17...\n",
      "Processing minute 18...\n",
      "Processing minute 19...\n",
      "Processing minute 20...\n",
      "Processing minute 21...\n",
      "Processing minute 22...\n",
      "Processing minute 23...\n",
      "Processing minute 24...\n",
      "Processing minute 25...\n",
      "Processing minute 26...\n",
      "Processing minute 27...\n",
      "Could not open segments/frame_1620_2.png: [Errno 2] No such file or directory: 'segments/frame_1620_2.png'\n",
      "Completed! Generated descriptions for 27 minutes.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Model Initialization\n",
    "def initialize_models():\n",
    "    processor = LlavaNextProcessor.from_pretrained(\n",
    "        \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        use_fast=True\n",
    "    )\n",
    "    \n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=None \n",
    "    ).to('cuda:0').eval()\n",
    "\n",
    "    model.generation_config.pad_token_id = model.config.eos_token_id\n",
    "    model.generation_config.eos_token_id = model.config.eos_token_id\n",
    "\n",
    "    print(f\"Model loaded on {next(model.parameters()).device}\")\n",
    "    print(f\"VRAM usage: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    \n",
    "    return processor, model\n",
    "\n",
    "# Extract 3 frames per 1-minute segment\n",
    "def extract_representative_frames(video_path, output_dir=\"segments\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    project_root = Path(__file__).parent\n",
    "    ffmpeg_path = project_root / \"ffmpeg\" / \"bin\" / \"ffmpeg.exe\"\n",
    "    \n",
    "    if not ffmpeg_path.exists():\n",
    "        raise FileNotFoundError(f\"FFmpeg not found at {ffmpeg_path}\")\n",
    "\n",
    "    subprocess.run([\n",
    "        str(ffmpeg_path),\n",
    "        \"-i\", video_path,\n",
    "        \"-vf\", \"select='(gt(scene,0.35))'\",\n",
    "        \"-vsync\", \"0\",\n",
    "        \"-frame_pts\", \"1\",\n",
    "        f\"{output_dir}/frame_%04d.png\"\n",
    "    ], check=True)\n",
    "    \n",
    "    frame_paths = sorted([str(p) for p in Path(output_dir).glob(\"frame_*.png\")])\n",
    "    return frame_paths\n",
    "\n",
    "# Process frames and summarize each segment\n",
    "def describe_frames_group(frames, processor, model):\n",
    "    images = []\n",
    "    for frame_path in frames:\n",
    "        try:\n",
    "            img = Image.open(frame_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not open {frame_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    texts = [\"[INST] <image>\\nDescribe this image. [/INST]\"] * len(images)\n",
    "    inputs = processor(images=images, text=texts, return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=100, do_sample=False\n",
    "        )\n",
    "\n",
    "    descriptions = [processor.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    combined_description = \" \".join(descriptions)\n",
    "    return combined_description.strip()\n",
    "\n",
    "# Save results\n",
    "def save_results(results, output_file=\"minute_descriptions.txt\"):\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for minute, desc in results:\n",
    "            f.write(f\"[Minute {minute}] {desc}\\n\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    processor, model = initialize_models()\n",
    "\n",
    "    video_path = \"30min_vid.mp4\"\n",
    "    frames_per_segment = extract_representative_frames(video_path)\n",
    "\n",
    "    results = []\n",
    "    for i, frame_path in enumerate(frames_per_segment):\n",
    "        print(f\"Processing minute {i}...\")\n",
    "        description = describe_frames_group([frame_path], processor, model)\n",
    "        if description:\n",
    "            results.append((i, description))\n",
    "\n",
    "    save_results(results)\n",
    "    print(f\"Completed! Generated descriptions for {len(results)} minutes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b750a-8d01-421a-9ddd-dcb48cffeba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as torch_func\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "similar_segments = []\n",
    "ground_truth = [\n",
    "                  [[0, 47], [48, 237], [238, 330], [331, 898], [899, 1204], [1205, 1499], [1500, 1662]],\n",
    "                  [[0, 31], [32, 404], [405, 623], [624, 1045], [1046, 1130], [1131, 1551], [1552, 1781]]\n",
    "                 ]\n",
    "text_labels = [[\"introduction\", \"recap: the surprise pi\", \"the game plan\", \"how to analyze the blocks\", \"the geometry puzzle\", \"small angle approximation\", \"the value of pure puzzles\"],\n",
    "               [\"introduction\", \"twirling ties\", \"tarski plank problem\", \"monge's theorem\", \"3d volume, 4d answer\", \"the hypercube stack\", \"the sadness of higher dimensions\"]]\n",
    "\n",
    "cosine_accuracy = {}\n",
    "\n",
    "def seconds_to_mmss(seconds):\n",
    "    minutes = int(seconds) // 60\n",
    "    remaining_seconds = int(seconds) % 60\n",
    "    return f\"{minutes:02}:{remaining_seconds:02}\"\n",
    "\n",
    "def calculate_iou(seg1, seg2):\n",
    "    start1, end1 = seg1\n",
    "    start2, end2 = seg2\n",
    "    intersection = max(0, min(end1, end2) - max(start1, start2))\n",
    "    union = max(end1, end2) - min(start1, start2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def compare_vectors(similarity_output):\n",
    "  for i in range(len(similarity_output) - 1):\n",
    "    vec1 = similarity_output[i]\n",
    "    vec2 = similarity_output[i + 1]\n",
    "\n",
    "    cos_sim = torch_func.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "    euclid_dist = torch.dist(vec1, vec2).item()\n",
    "\n",
    "    if (cos_sim > 0.85 and euclid_dist < 0.2):\n",
    "      similar_segments.append([i, i+1])\n",
    "    # print(f\"Comparing row {i} & {i+1}:\")\n",
    "    # print(f\"  Cosine Similarity: {cos_sim:.4f}\")\n",
    "    # print(f\"  Euclidean Distance: {euclid_dist:.4f}\")\n",
    "\n",
    "def combine_segments(current_segments, combine_segments):\n",
    "  # print(f\"Segments to combine: {combine_segments}\")\n",
    "  from collections import defaultdict\n",
    "\n",
    "  adjacency = defaultdict(set)\n",
    "  for i, j in combine_segments:\n",
    "      adjacency[i].add(j)\n",
    "      adjacency[j].add(i)\n",
    "\n",
    "  visited = set()\n",
    "  groups = []\n",
    "\n",
    "  for node in sorted(adjacency.keys()):\n",
    "      if node not in visited:\n",
    "          stack = [node]\n",
    "          group = []\n",
    "\n",
    "          while stack:\n",
    "              current = stack.pop()\n",
    "              if current not in visited:\n",
    "                  visited.add(current)\n",
    "                  group.append(current)\n",
    "                  stack.extend(adjacency[current])\n",
    "\n",
    "          if len(group) > 1:\n",
    "              groups.append(sorted(group))\n",
    "\n",
    "  segments_to_remove = set()\n",
    "  for group in groups:\n",
    "      start_idx = group[0]\n",
    "      end_idx = group[-1]\n",
    "\n",
    "      current_segments[start_idx] = [current_segments[start_idx][0], current_segments[end_idx][1]]\n",
    "\n",
    "      for idx in group[1:]:\n",
    "          segments_to_remove.add(idx)\n",
    "\n",
    "  for idx in sorted(segments_to_remove, reverse=True):\n",
    "      del current_segments[idx]\n",
    "\n",
    "  return current_segments\n",
    "\n",
    "def analyze_thresholds(image_features, ground_truth, thresholds=np.linspace(0.7, 0.95, 20)):\n",
    "    segment_counts = []\n",
    "    segment_frequency = np.zeros(len(image_features))  # Tracks how often each segment appears\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        temp_similar_segments = []\n",
    "        for i in range(len(image_features) - 1):\n",
    "            vec1 = image_features[i]\n",
    "            vec2 = image_features[i + 1]\n",
    "            cos_sim = torch_func.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "            if cos_sim > threshold:\n",
    "                temp_similar_segments.append([i, i + 1])\n",
    "                segment_frequency[i] += 1\n",
    "                segment_frequency[i + 1] += 1\n",
    "\n",
    "        temp_segments = [[i * 60, (i + 1) * 60] for i in range(len(image_features))]\n",
    "        merged = combine_segments(temp_segments.copy(), temp_similar_segments)\n",
    "        print(f\"\\nCosine threshold: {threshold:.2f}\")\n",
    "        segment_counts.append(len(merged))\n",
    "\n",
    "        matches = 0\n",
    "        ious = []\n",
    "        for gt in ground_truth:\n",
    "            best_iou = 0\n",
    "            for merged_seg in merged:\n",
    "                iou = calculate_iou(gt, merged_seg)\n",
    "                best_iou = max(best_iou, iou)\n",
    "            ious.append(best_iou)\n",
    "            if best_iou > 0.5:\n",
    "                matches += 1\n",
    "\n",
    "        accuracy = round((matches / len(ground_truth)), 2)\n",
    "        threshold = round(threshold, 2)\n",
    "        cosine_accuracy.setdefault(threshold, []).append(accuracy)\n",
    "        print(f\"IoUs: {[f'{iou:.2f}' for iou in ious]}\")\n",
    "        # print(f\"Matched Segments: {matches}/{len(ground_truth)}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # # Plot: Threshold vs Final Segment Count\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.plot(thresholds, segment_counts, marker='o')\n",
    "    # plt.xlabel(\"Cosine Similarity Threshold\")\n",
    "    # plt.ylabel(\"Number of Segments After Merging\")\n",
    "    # plt.title(\"Impact of Cosine Threshold on Segment Count\")\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    # # Plot: Segment Similarity Frequency\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # x_labels = [f\"{seconds_to_mmss(i*60)}\" for i in range(len(segment_frequency))]\n",
    "    # plt.bar(range(len(segment_frequency)), segment_frequency, tick_label=x_labels)\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    # plt.xlabel(\"Segment Start Time\")\n",
    "    # plt.ylabel(\"Similarity Frequency\")\n",
    "    # plt.title(\"Segment Similarity Frequency Across Thresholds\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Optional: Return which segment(s) are most similar\n",
    "    # top_segments = np.argsort(segment_frequency)[-3:][::-1]  # top 3 segments\n",
    "    # print(\"Most clustered segments:\")\n",
    "    # for idx in top_segments:\n",
    "    #     print(f\"Segment {idx} ({seconds_to_mmss(idx*60)} - {seconds_to_mmss((idx+1)*60)}), Frequency: {segment_frequency[idx]}\")\n",
    "\n",
    "def load_clip_text_inputs_from_file(file_path=\"minute_descriptions.txt\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        descriptions = []\n",
    "        for line in f:\n",
    "            if \"]\" in line:\n",
    "                description = line.strip().split(\"]\", 1)[-1].strip()\n",
    "                if description:\n",
    "                    descriptions.append(description)\n",
    "        return descriptions\n",
    "        \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "video_path = [\"30min_vid.mp4\"]\n",
    "\n",
    "def run_evaluation_model(video_path):\n",
    "  for video_idx, video in enumerate(video_path):\n",
    "    print(f\"Video idx: {video_idx}; video_path: {video}\")\n",
    "    cap = cv2.VideoCapture(video)\n",
    "\n",
    "    # calculate how many frames per 1m segment & how many segments\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration_sec = total_frames / fps\n",
    "    num_segments = int(duration_sec // 60)\n",
    "\n",
    "    frames_per_segment = int(fps * 60)\n",
    "\n",
    "    random_frames = []\n",
    "    segments = []\n",
    "\n",
    "    # loop for choosing random frame\n",
    "    for segment_idx in range(num_segments):\n",
    "        start_frame = segment_idx * frames_per_segment\n",
    "        end_frame = start_frame + frames_per_segment\n",
    "\n",
    "        start_time, end_time = start_frame / fps, end_frame / fps\n",
    "        segments.append([start_time, end_time])\n",
    "        # Random frame from segment\n",
    "        random_frame_index = random.randint(start_frame, min(end_frame - 1, total_frames - 1))\n",
    "\n",
    "        # Get index and append to frame array for querying\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert BGR (OpenCV) to RGB (PIL)\n",
    "            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            random_frames.append((img, preprocess(img)))\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # batch processing\n",
    "    image_inputs = torch.stack([p for _, p in random_frames])\n",
    "    image_inputs = image_inputs.to(device)\n",
    "\n",
    "    descriptions = load_clip_text_inputs_from_file(\"minute_descriptions.txt\")\n",
    "    text_inputs = clip.tokenize(descriptions, truncate=True)\n",
    "    text_inputs = text_inputs.to(device)\n",
    "\n",
    "    # cosine similarity of text query and image feature/video frame\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "        print(similarity)\n",
    "\n",
    "    for i in range(3):\n",
    "      print(f\"ground truth: {ground_truth[video_idx]}\")\n",
    "      analyze_thresholds(image_features, ground_truth=ground_truth[video_idx])\n",
    "\n",
    "    for cosine_threshold in cosine_accuracy:\n",
    "      average_accuracy = np.mean(cosine_accuracy[cosine_threshold])\n",
    "      print(f\"Cosine threshold: {cosine_threshold:.2f}; Average Accuracy: {average_accuracy:.2f}\")\n",
    "      # print(f\"Average Accuracy: {average_accuracy}\")\n",
    "      # for accuracy in cosine_accuracy[cosine_threshold]:\n",
    "      #   print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "run_evaluation_model(video_path)\n",
    "\n",
    "# compare_vectors(similarity)\n",
    "# print(combine_segments(segments, similar_segments))\n",
    "\n",
    "# percentages = (similarity.to(torch.float32) * 100).to(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
